{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sxPJRpoc3S-"
   },
   "outputs": [],
   "source": [
    "# preprocessing step is the most important step in ML modelling.\n",
    "# in preprocessing step, we prepare datasets which will be used in create ML Model.\n",
    "# two main step in data preprocessing :\n",
    "# 1. Importing the dataset \n",
    "# 2. splitting the dataset into train set and test set : 2 main dataset include feature & label\n",
    "\n",
    "# we may be used other step depend on what kind of data \n",
    "# encoding categorical data : transform the string based value to number, because ML can't process string data\n",
    "# fill missing data : filling null data with some strategy (ex: mean of all other value)\n",
    "# feature scaling : transform unscaled data and make sure all data have same scale \n",
    "#                   (ex: amount and ages data exist on same dataset, it must be transformed to same scale)\n",
    "\n",
    "# *Feature is independant variable, variable that determine label\n",
    "# *label is dependant variable, variable that determined by feature"
   ]
  },
  {
   "source": [
    "## Importing the libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Iu9Z_nxugg4"
   },
   "outputs": [],
   "source": [
    "# this is the basic library for data processing in python\n",
    "import numpy as np # for work with array\n",
    "import matplotlib.pyplot as pl # for make graph chart & visualize data\n",
    "import pandas as pd # for import & make matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CJZSvJaFvqW3"
   },
   "outputs": [],
   "source": [
    "# import and create data frame to process\n",
    "# pandas automatically make first row as col name\n",
    "dataset = pd.read_csv('01_preprocessing_data.csv') \n",
    "\n",
    "# container for matrix of feature (feature is column used to make prediction)\n",
    "# use function iloc to get location of feature col (1-3)(all except last one)\n",
    "# first one is extract row, second one is extract column (imagine the dataset is formed like 2d matrix)\n",
    "# if we use 1d matrix, we just need [:], if 2d [:, :]\n",
    "x_feature = dataset.iloc[:, :-1].values\n",
    "\n",
    "# container for dependant variable (depandant is what ML want to predict)\n",
    "y_dependant = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1n7zz0eHzzMN"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Country   Age   Salary Purchased\n0   France  44.0  72000.0        No\n1    Spain  27.0  48000.0       Yes\n2  Germany  30.0  54000.0        No\n3    Spain  38.0  61000.0        No\n4  Germany  40.0      NaN       Yes\n5   France  35.0  58000.0       Yes\n6    Spain   NaN  52000.0        No\n7   France  48.0  79000.0       Yes\n8  Germany  50.0  83000.0        No\n9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KLdH-I4r2dbB"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 nan]\n ['France' 35.0 58000.0]\n ['Spain' nan 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SBz9G3vA2fDL"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y_dependant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ohOm35l_jeE5"
   },
   "outputs": [],
   "source": [
    "# change all missing data with average of all other row in same column\n",
    "\n",
    "# use scikitlearn module, most of common used module in datascience & ML (read imputer)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# initiate class of imputer\n",
    "# set parameter missing_values to not found cell (nan value), to declare nan values use np.nan\n",
    "# set parameter strategy to what can of action we want to do if missing_value is found\n",
    "var_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# add pre transformed matrix to imputer\n",
    "# select all x_feature row, on column age & salary (column age is on index 1 & 2, so slice on 1:3, from index 1 to before index 3)\n",
    "var_imputer.fit(x_feature[:, 1:3])\n",
    "\n",
    "# execute transformation and put result on original matrix\n",
    "x_feature[:, 1:3] = var_imputer.transform(x_feature[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p1-nGhENr6un"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsrUDIyyuZW_"
   },
   "outputs": [],
   "source": [
    "# we have 2 column of data that written in string (country and purchased), \n",
    "# ML can't recognize string value, so we must encode it to number  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "krKL67g9wmI9"
   },
   "outputs": [],
   "source": [
    "# for country column\n",
    "# first idea is turn france = 0, spain = 1, germany = 2, but if we do this \n",
    "# ML can interpret this column as ordered number.\n",
    "# Other idea is one-hot encoding(let's google it), by make 3 column because we have 3 category\n",
    "# so french = 100, spain 010, germany = 001\n",
    "\n",
    "# to one-hot encoding use 2 clases\n",
    "# column transfrom class\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# one-hot class\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# declare object column transformer\n",
    "# transformer : what kind of transformation, what kind of encoding, index column (in tuple)\n",
    "# remainder : parameter to keep or not to keep others column that not transformed\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "# fit & transform the matrix and keep it in old variable\n",
    "# and make result as numpy array because ML use numpy array\n",
    "x_feature = np.array(ct.fit_transform(x_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFFX6bA5zxEe",
    "outputId": "c7298e78-9a63-4961-c0d0-77b64643ec64"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "\n",
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1ZIPY4L00YoD"
   },
   "outputs": [],
   "source": [
    "# for purchased column\n",
    "# we don't need to one-hot this column because it only have 2 category\n",
    "# so we just need to label it\n",
    "\n",
    "# import label encoder class\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# declare object label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and transform matrix\n",
    "# label encoder don't need transform to np array because it already np array\n",
    "y_dependant = le.fit_transform(y_dependant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXgbo4471URp",
    "outputId": "e5110dc4-12ed-4503-96f7-aae96d84b23f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_dependant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Pp3HxUBLRcnU"
   },
   "outputs": [],
   "source": [
    "# training set : set of data that used to train the ML model\n",
    "# test set : set of data that used to test the accuracy of ML model\n",
    "# we use sklearn module to split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 4 container for result, x train, y train, x test, y test\n",
    "# train_test_split use x_feature and y_dependant as parameter, not a complete dataset\n",
    "# test_size : size of test part in scale 0-1, if we make it 0.3 then the train set will be 0.7\n",
    "# random_state : make sure selection of set will be random\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_feature, y_dependant, test_size = 0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yxoq5fRWhll",
    "outputId": "76f516b8-ba7a-4aa6-c8f1-fef89cdb1951"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wj_C-gGhWhtW",
    "outputId": "5acd8a98-f152-40a7-dcfe-d6f02aac13d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TN06fQDNWh0r",
    "outputId": "1bf42f4e-7f4e-412b-f600-a732adf9d8a8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dx7R4LeWh6w",
    "outputId": "af913bfe-596c-4bf0-8a28-c05f3c4a52a1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EDnK5gEPRq86"
   },
   "outputs": [],
   "source": [
    "# when feature scaling done is still debatable\n",
    "# this tutorial recommend to do feature scaling after splitting dataset\n",
    "# because test set is supposed to be a new brand set that tested after model deployed\n",
    "# and feature scaling is done by using all the dataset to fit. \n",
    "# Logically, the test set must follow the model scale, not determine the model scale.\n",
    "# Both test & train data must be scale, but only train data that allowed to determine the model scale.\n",
    "\n",
    "# feature scaling is used to scaling all feature to make sure that all feature have same scale\n",
    "# and there isn't a feature that dominate other feature\n",
    "# 2 common scaling technique\n",
    "# standardisation : x-mean(x)/ standard deviation of x, resulting value between -3 to +3\n",
    "# normalisation : x - min(x)/ max(x) - min(x), resulting value 0 to 1\n",
    "# normalisation is used when we have normal distribution in most feature\n",
    "# standardisation work well all the time \n",
    "\n",
    "# we only use feature scaling on uncategorized variable\n",
    "# because we already encoded category to 3 values, if we do scaling on this feature, we will lost it information\n",
    "\n",
    "# we use standardisation on this tutorial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# declare object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# get only 2 last column (from col 3)\n",
    "# we make a scale by fitting the train test & transform it values to determined scale\n",
    "x_train[:, 3:] = sc.fit_transform((x_train[:, 3:]))\n",
    "# transform test data to determined scale\n",
    "x_test[:, 3:] = sc.transform((x_test[:, 3:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efO0g19oclM8",
    "outputId": "7cc55305-d717-4c79-ddc0-5576342cde96"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhVVTyOKclQD",
    "outputId": "aef0f654-577d-4ddc-c7b9-c38f6d0c617b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01_data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}