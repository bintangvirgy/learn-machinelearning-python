{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_support_vector_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangvirgy/learn-machinelearning-python/blob/main/05_support_vector_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3PAEPRDRLA3"
      },
      "source": [
        "# Support Vector Regression (SVR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mQ0P-uOBo-D"
      },
      "source": [
        "# This training will talk about linear SVR.\n",
        "# In SLR, we search the minimum value (ordinary least squares) from data to prediction\n",
        "# In SVR, same as SLR, but instead of line, we will make tube epsilon insensitive tube, that have thickness (width) called epsilon\n",
        "# the width is represent margin of error that the model allowed to had.\n",
        "# SLR care about all point that not placed in SLR line, but SVR only care all point that not placed inside the tube.\n",
        "\n",
        "# SVR also support non linear regression to, depend on what kind of kernel \n",
        "\n",
        "# even we have same dataset as polynomial training, in this chapter we must do feature scaling.\n",
        "# because SVR don't have explicit equation, so it don't have coefficient like polynomial\n",
        "# then, we must make implisit feature scaling to each dependant variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VCUAVIjRdzZ"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZr3ByxiBaSe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXVXoFWtSF4_"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v6BgtYOBbN7",
        "outputId": "9ed3d27e-32a8-47ed-9b85-450e9e7ce332"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCVqVDBKBb0L"
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/Learn_Machinelearning_Udemy/05.SVR/Position_Salaries.csv')\n",
        "x = dataset.iloc[:,1:-1].values\n",
        "y = dataset.iloc[:,-1:].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7A2rjMPjCSl",
        "outputId": "fe15d3d0-b43c-4352-eb77-c9f3c472fe51"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [ 8]\n",
            " [ 9]\n",
            " [10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdCUP06WjDZV",
        "outputId": "04c3f10c-5bfb-40e2-d8e1-38604475ab7d"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  45000]\n",
            " [  50000]\n",
            " [  60000]\n",
            " [  80000]\n",
            " [ 110000]\n",
            " [ 150000]\n",
            " [ 200000]\n",
            " [ 300000]\n",
            " [ 500000]\n",
            " [1000000]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8FeLHYS-nI"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOUBjmvHYcf4"
      },
      "source": [
        "# this step will cover how to do feature scaling for data preprocess & how to reverse the preprocessed feature scaling\n",
        "# all dataset will be used, because we want full accuracy based on data.\n",
        "# in training 1, we only scale the independant variable because the dependant variable was take value 0/1\n",
        "# in this training, we have different situation, where feature is range from 1-10, and dependant variable is salary data (45000 - 1000000)\n",
        "# we must do feature scaling on both variable, using standardisation\n",
        "# standardisation used separately because independant & dependant data have different scale, so if we use same object of fitted x to transform the Y, Y will use mean of x\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# option 1, we can concatenate x and y, do standardisation, and split it again\n",
        "sc = StandardScaler()\n",
        "combine = np.concatenate((x,y),axis=1)\n",
        "combine_scl = sc.fit_transform(combine)\n",
        "comb_x = combine_scl[:,:-1]\n",
        "comb_y = combine_scl[:,-1:]\n",
        "\n",
        "# option 2, use 2 standardscaler object to fit \n",
        "scx = StandardScaler()\n",
        "x_scl = scx.fit_transform(x)\n",
        "\n",
        "scy = StandardScaler()\n",
        "y_scl = scy.fit_transform(y)\n",
        "\n",
        "# but better use option 2, because after we standardize each feature, if we want to predict some value, we must do transform to x val, and do reverse transform to y result"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inGDfk4Moh0q",
        "outputId": "830f474d-198a-4b6e-834e-2bf88451cd62"
      },
      "source": [
        "print(combine)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[      1   45000]\n",
            " [      2   50000]\n",
            " [      3   60000]\n",
            " [      4   80000]\n",
            " [      5  110000]\n",
            " [      6  150000]\n",
            " [      7  200000]\n",
            " [      8  300000]\n",
            " [      9  500000]\n",
            " [     10 1000000]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB53rZ_KpJF1",
        "outputId": "87efc662-c2ba-47fe-f302-921e49a2a623"
      },
      "source": [
        "print(combine_scl)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.5666989  -0.72004253]\n",
            " [-1.21854359 -0.70243757]\n",
            " [-0.87038828 -0.66722767]\n",
            " [-0.52223297 -0.59680786]\n",
            " [-0.17407766 -0.49117815]\n",
            " [ 0.17407766 -0.35033854]\n",
            " [ 0.52223297 -0.17428902]\n",
            " [ 0.87038828  0.17781001]\n",
            " [ 1.21854359  0.88200808]\n",
            " [ 1.5666989   2.64250325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELgtJQa2qLFv",
        "outputId": "820d316d-5f4c-499d-9e1b-e856d95bdaf4"
      },
      "source": [
        "print(x_scl)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.5666989 ]\n",
            " [-1.21854359]\n",
            " [-0.87038828]\n",
            " [-0.52223297]\n",
            " [-0.17407766]\n",
            " [ 0.17407766]\n",
            " [ 0.52223297]\n",
            " [ 0.87038828]\n",
            " [ 1.21854359]\n",
            " [ 1.5666989 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRXEBEo2qOgw",
        "outputId": "108ebaf5-ea4c-4add-b87a-181bcadf9c1f"
      },
      "source": [
        "print(y_scl)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.72004253]\n",
            " [-0.70243757]\n",
            " [-0.66722767]\n",
            " [-0.59680786]\n",
            " [-0.49117815]\n",
            " [-0.35033854]\n",
            " [-0.17428902]\n",
            " [ 0.17781001]\n",
            " [ 0.88200808]\n",
            " [ 2.64250325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiU6D2QFRjxY"
      },
      "source": [
        "## Training the SVR model on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhXdTaJ9woIp"
      },
      "source": [
        "# import module SVR from scikit learn\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# create object svr\n",
        "# in svr, we must input parameter, \n",
        "# must define : kernel (what kind of SVR)\n",
        "# trainer recommend use RBF kernel each time we do experimental procedure use SVR\n",
        "sv = SVR(kernel = 'rbf')\n",
        "\n",
        "# train model use dataset\n",
        "sv.fit(x_scl,y_scl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deDnDr8UR5vq"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aesYyJXwzUW2"
      },
      "source": [
        "# to reverse scaled value, use inverse transform on standardisation object that already fitted.\n",
        "reverse_y = scy.inverse_transform(y_scl)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Mbr3ERzcJR",
        "outputId": "e5fd7a09-2254-45fe-fceb-74165c7efe7c"
      },
      "source": [
        "print(reverse_y)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  45000.]\n",
            " [  50000.]\n",
            " [  60000.]\n",
            " [  80000.]\n",
            " [ 110000.]\n",
            " [ 150000.]\n",
            " [ 200000.]\n",
            " [ 300000.]\n",
            " [ 500000.]\n",
            " [1000000.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzedFlUISSu_"
      },
      "source": [
        "## Visualising the SVR results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UahPVNlJSZ-K"
      },
      "source": [
        "## Visualising the SVR results (for higher resolution and smoother curve)"
      ]
    }
  ]
}